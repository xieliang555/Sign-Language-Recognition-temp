{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from skimage import io\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "class PhoenixDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_dir, root_dir, transforms):\n",
    "        self.csv_file = pd.read_csv(csv_dir)\n",
    "        self.root_dir = root_dir\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if(torch.is_tensor(idx)):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        clip_path = os.path.join(\n",
    "            self.root_dir, self.csv_file.iloc[idx, 0].split('|')[1])\n",
    "        collection = io.imread_collection(clip_path)\n",
    "        clip = torch.zeros(len(collection), 3, 224, 224)\n",
    "        for i, img in enumerate(collection):\n",
    "            clip[i, :, :, :] = self.transforms(img)\n",
    "\n",
    "        annotation = self.csv_file.iloc[idx, 0].split('|')[3]\n",
    "        sample = {'clip': clip, 'annotation': annotation}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def statistic_analysis(self):\n",
    "        '''\n",
    "        analysis the dataset\n",
    "        '''\n",
    "        size = len(self.csv_file)\n",
    "        annotations_len = [len(self.csv_file.iloc[i, 0].split('|')[\n",
    "                               3].split()) for i in range(size)]\n",
    "        print('corpus pairs: {}\\n'.format(size))\n",
    "        print('max_anotation_length: {}'.format(max(annotations_len)))\n",
    "        print('min_anotation_length: {}'.format(min(annotations_len)))\n",
    "        print('ave_anotation_length: {}\\n'.format(sum(annotations_len)/size))\n",
    "        \n",
    "        clip_path = os.path.join(\n",
    "            self.root_dir, self.csv_file.iloc[0, 0].split('|')[1])\n",
    "        collection = io.imread_collection(clip_path)\n",
    "        frame_size = collection[0].shape\n",
    "        print('frame_size: {}'.format(frame_size))\n",
    "        \n",
    "        clip_pathes = [os.path.join(self.root_dir, self.csv_file.iloc[i, 0].split('|')[\n",
    "                                    1]) for i in range(size)]\n",
    "        clip_len = [len(glob.glob(i)) for i in clip_pathes]\n",
    "        print('max_clip_length: {}'.format(max(clip_len)))\n",
    "        print('min_clip_length: {}'.format(min(clip_len)))\n",
    "        print('ave_clip_length: {}'.format(sum(clip_len)/size))\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "\n",
    "#     csv_root = '/media/xieliang555/新加卷/数据集/phoenix2014-release/phoenix-2014-multisigner/annotations/manual'\n",
    "#     clip_root = '/media/xieliang555/新加卷/数据集/phoenix2014-release/phoenix-2014-multisigner/features/fullFrame-210x260px'\n",
    "\n",
    "#     # test set\n",
    "#     print('================ test set ==============')\n",
    "#     test_csv_dir = os.path.join(csv_root, 'test.corpus.csv')\n",
    "#     test_root_dir = os.path.join(clip_root, 'test')\n",
    "#     test_set = PhoenixDataset(test_csv_dir, test_root_dir, transforms=None)\n",
    "#     test_set.statistic_analysis()\n",
    "\n",
    "#     # dev set\n",
    "#     print('================ dev set ==============')\n",
    "#     dev_csv_dir = os.path.join(csv_root, 'dev.corpus.csv')\n",
    "#     dev_root_dir = os.path.join(clip_root, 'dev')\n",
    "#     dev_set = PhoenixDataset(dev_csv_dir, dev_root_dir, transforms=None)\n",
    "#     dev_set.statistic_analysis()\n",
    "\n",
    "#     # training set\n",
    "#     print('================ train set ==============')\n",
    "#     train_csv_dir = os.path.join(csv_root, 'train.corpus.csv')\n",
    "#     train_root_dir = os.path.join(clip_root, 'train')\n",
    "#     train_set = PhoenixDataset(train_csv_dir, train_root_dir, transforms=None)\n",
    "#     train_set.statistic_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sign]",
   "language": "python",
   "name": "conda-env-sign-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
