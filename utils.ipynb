{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T06:58:12.711736Z",
     "start_time": "2020-02-27T06:58:12.702075Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data import Field, TabularDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DatasetStatistic(root, mode='train'):\n",
    "    '''\n",
    "    analysis the statistic of the RWTH-PHOENIX-Weather 2014 dataset\n",
    "    Args:\n",
    "        root: the whole data root\n",
    "        mode: 'train','dev', 'test'\n",
    "    return: \n",
    "        max_len: maximum video length \n",
    "        min_len: minimum video length \n",
    "        ave_len: average video length \n",
    "        std_len: std video length\n",
    "    '''\n",
    "    root = os.join(root, 'phoenix2014-release/phoenix-2014-multisigner')\n",
    "    video_root = os.join(root, 'features/fullFrame-210x260px' + mode)\n",
    "    csv_path = os.join(root, 'annotations/manual/' + mode + '.corpus.csv')\n",
    "\n",
    "    csv_file = pd.read_csv(csv_path)\n",
    "    video_paths = [os.join(video_root, csv_path.iloc[i, 0].split('|')[1])\n",
    "                   for i in range(csv_file.shape[0])]\n",
    "\n",
    "    video_lens = [len(glob.glob(path)) for path in video_paths]\n",
    "    return [max(video_lens), min(video_lens),\n",
    "            np.mean(video_lens), np.std(video_lens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/mnt/data/public/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def process_annotations(annotations, csv_dir='/media/xieliang555/新加卷/数据集/phoenix2014-release/phoenix-2014-multisigner/annotations/manual'):\n",
    "    '''\n",
    "            this function pad and numericalize the annotation batch\n",
    "    '''\n",
    "    if not os.path.exists('.data/train.annotations.csv'):\n",
    "        csv_file = pd.read_csv(os.path.join(csv_dir, 'train.corpus.csv'))\n",
    "        data = [csv_file.iloc[i, 0].split('|')[3]\n",
    "                for i in range(len(csv_file))]\n",
    "        f = open('.data/train.annotations.csv', 'a')\n",
    "        for annotation in data:\n",
    "            f.write(annotation+'\\n')\n",
    "        f.close()\n",
    "\n",
    "    if not os.path.exists('.data/dev.annotations.csv'):\n",
    "        csv_file = pd.read_csv(os.path.join(csv_dir, 'dev.corpus.csv'))\n",
    "        data = [csv_file.iloc[i, 0].split('|')[3]\n",
    "                for i in range(len(csv_file))]\n",
    "        f = open('.data/dev.annotations.csv', 'a')\n",
    "        for annotation in data:\n",
    "            f.write(annotation+'\\n')\n",
    "        f.close()\n",
    "\n",
    "    if not os.path.exists('.data/test.annotations.csv'):\n",
    "        csv_file = pd.read_csv(os.path.join(csv_dir, 'test.corpus.csv'))\n",
    "        data = [csv_file.iloc[i, 0].split('|')[3]\n",
    "                for i in range(len(csv_file))]\n",
    "        f = open('.data/test.annotations.csv', 'a')\n",
    "        for annotation in data:\n",
    "            f.write(annotation+'\\n')\n",
    "        f.close()\n",
    "\n",
    "    TRG = Field(tokenize=\"spacy\",\n",
    "                tokenizer_language=\"de\",\n",
    "                init_token='<sos>',\n",
    "                eos_token='<eos>',\n",
    "                lower=True)\n",
    "\n",
    "    train_set, dev_set, test_set = TabularDataset.splits(path='.data/',\n",
    "                                                         train='train.annotations.csv', validation='dev.annotations.csv',\n",
    "                                                         test='test.annotations.csv', format='csv', fields=[(\"TRG\", TRG)])\n",
    "\n",
    "    TRG.build_vocab(train_set, min_freq=2)\n",
    "\n",
    "    return TRG.process(annotations)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "            this function pad the variant video sequence length to the\n",
    "            fixed length and preprocess the annotations for the DataLoader\n",
    "    '''\n",
    "    clips = [item['clip'] for item in batch]\n",
    "    annotations = [item['annotation'].lower().split() for item in batch]\n",
    "    clips_padded = pad_sequence([clip for clip in clips], batch_first=True)\n",
    "    annotations = process_annotations(annotations)\n",
    "\n",
    "    return {'clip': clips_padded, 'annotation': annotations}\n",
    "\n",
    "\n",
    "def itos(annotations):\n",
    "    \"\"\"\n",
    "            transform numerica to text\n",
    "            this is the reverse function of process_annotations\n",
    "    \"\"\"\n",
    "    TRG = Field(tokenize='spacy',\n",
    "                tokenizer_language='de',\n",
    "                init_token='<sos>',\n",
    "                eos_token='<eos>',\n",
    "                lower=True)\n",
    "\n",
    "    train_set, dev_set, test_set = TabularDataset.splits(path='.data/',\n",
    "                                                         train='train.annotations.csv', validation='dev.annotations.csv',\n",
    "                                                         test='test.annotations.csv', format='csv', fields=[('TRG', TRG)])\n",
    "\n",
    "    TRG.build_vocab(train_set, min_freq=2)\n",
    "\n",
    "    annotations = annotations.transpose(1, 0)\n",
    "    text = [[TRG.vocab.itos[i] for i in annotation]\n",
    "            for annotation in annotations]\n",
    "\n",
    "    return text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sign]",
   "language": "python",
   "name": "conda-env-sign-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
